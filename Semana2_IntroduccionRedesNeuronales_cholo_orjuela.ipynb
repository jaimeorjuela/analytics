{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><h1 align=\"Center\"> <img src=\"https://serea2017.uniandes.edu.co/images/Logo.png\" height=\"60\" width=\"200\" align=\"Center\" />MIIA-4203 MODELOS AVANZADOS PARA ANÁLISIS DE DATOS II</h1>\n",
    "<h2 align=\"Center\">\n",
    "Presentado por:<br>\n",
    "Jaime Orjuela Viracacha - Cód 201924252<br>\n",
    "Fabián Cholo Acevedo - Cód 201523509<br>\n",
    "</h2>\n",
    "<h3 align=\"Center\">Introducción a las redes neuronales\n",
    "\n",
    "Actividad 2\n",
    "\n",
    "Profesor: Camilo Franco (c.franco31@uniandes.edu.co) </h3>\n",
    "\n",
    "\n",
    "\n",
    "En esta actividad vamos a estudiar una primera aproximación a los modelos de redes neuronales, utilizando como base el modelo de regresión logística.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algunos paquetes iniciales que vamos a utilizar\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Problema de clasificación: riesgo de default\n",
    "\n",
    "Examinemos los datos con lo cuales ya estamos familiarizados:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_1 = pd.read_csv(\"germancredit.csv\")\n",
    "credit_1 = pd.get_dummies(credit_1, columns=['checkingstatus1','history','purpose','savings',\n",
    "                                   'employ','status','others','property','otherplans','housing','job','tele', \n",
    "                                   'foreign'], prefix = ['checkingstatus1','history','purpose','savings',\n",
    "                                   'employ','status','others','property','otherplans','housing','job','tele', \n",
    "                                   'foreign'])\n",
    "X = credit_1.iloc[:, 1:62]\n",
    "Y = credit_1.iloc[:, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño de CE, CP:  (600,) (400,)\n",
      "Observaciones de la clase positiva en entrenamiento: 180 y en prueba: 120\n"
     ]
    }
   ],
   "source": [
    "CE_x, CP_x, CE_y, CP_y = model_selection.train_test_split(X, Y, test_size=0.4, random_state=42, stratify=Y)\n",
    "print(\"Tamaño de CE, CP: \", CE_y.shape, CP_y.shape)\n",
    "print(\"Observaciones de la clase positiva en entrenamiento: \" +str(sum(CE_y)) +\" y en prueba: \" +str(sum(CP_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Construcción de una neurona Sigmoide\n",
    "\n",
    "Una neurona Sigmoide puede ser vista como un perceptrón *suavizado* que recibe una señal y entonces se activa. Al activarse, transforma la señal para entender mejor el mensaje. Esta transformación la ejecuta a partir de la función Sigmoide.\n",
    "\n",
    "Si tomamos la señal como un conjunto de datos de entrada y el mensaje como la predicción de un valor, la función de activación jugará el papel de transformadora de los datos de entrada en aquello que se quiere entender/predecir, que además replica un modelo logit con la función de activación sigmoide.\n",
    "\n",
    "A continuación construiremos un clasificador de regresión logística bajo la perspectiva de una red neuronal, estudiando la arquitectura general de un algoritmo de aprendizaje. De esta manera, necesitaremos incluir la inicialización de los parámetros, el cálculo de la función de coste y su gradiente, y utilizar un algoritmo de optimización como por ejemplo el descenso en la dirección del gradiente (GD)\n",
    "\n",
    "**Formulación del algoritmo**:\n",
    "\n",
    "Para un ejemplo $x^{(i)}$:\n",
    "$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n",
    "$$\\hat{y}^{(i)} = a^{(i)} = sigmoide(z^{(i)})\\tag{2}$$ \n",
    "$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}$$\n",
    "\n",
    "El coste se calcula sumando sobre todos los ejemplos de entrenamiento:\n",
    "$$ L = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{4}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construimos las partes del algoritmo  \n",
    "\n",
    "- Inicializar los parámetros del modelo\n",
    "- Bucle:\n",
    "    - Calcular la pérdida actual (propagación hacia delante)\n",
    "    - Calcular el gradiente actual (retro-propagación)\n",
    "    - Actualizar los parámetros (descenso en la dirección del gradiente)\n",
    "\n",
    "\n",
    "### Ejercicio 2.1\n",
    "Implemente la funcion `sigmoide()` $$\\sigma( w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}$$ Para ello puede utilizar np.exp()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoide(z):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    z: Un escalar o arreglo numpy de cualquier tamaño\n",
    "    Output:\n",
    "    s: sigmoid(z)\n",
    "    \"\"\"\n",
    "\n",
    "    s = 1/(1+np.exp(-z))\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/albahnsen/PracticalMachineLearningClass/master/notebooks/images/logistic_function.png\" width=\"65%\" />\n",
    "\n",
    "_(Source: Python Machine Learning, S. Raschka)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoide([99,1,0,-1,-99]) = [1.00000000e+00 7.31058579e-01 5.00000000e-01 2.68941421e-01\n",
      " 1.01122149e-43]\n"
     ]
    }
   ],
   "source": [
    "print (\"sigmoide([99,1,0,-1,-99]) = \" + str(sigmoide(np.array([99,1,0,-1,-99]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada**: \n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td> sigmoide([99,1,0,-1,-99])    = </td>\n",
    "<td> [ 1.00000000e+00 7.31058579e-01 5.00000000e-01 2.68941421e-01\n",
    " 1.01122149e-43] </td> \n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2.2 \n",
    "\n",
    "Debemos inicializar los parámetros a cero. Puede utilizar la funcion np.zeros(), apoyandose en la documentación de la biblioteca Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inicializa_ceros(dim):\n",
    "    \"\"\"\n",
    "    Esta función crea un vector de ceros de dimensión (dim, 1) para w e inicializa b a 0.\n",
    "    Input:\n",
    "    dim: tamaño del vector w (número de parámetros para este caso)\n",
    "    Output:\n",
    "    w: vector inicializado de tamaño (dim, 1)\n",
    "    b: escalar inicializado (corresponde con el sesgo)\n",
    "    \"\"\"\n",
    "    \n",
    "    w = np.zeros(shape=(dim,1))\n",
    "    b = 0\n",
    "    \n",
    "    assert(w.shape == (dim, 1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "b = 0\n"
     ]
    }
   ],
   "source": [
    "dim = 6\n",
    "w, b = inicializa_ceros(dim)\n",
    "print (\"w = \" + str(w))\n",
    "print (\"b = \" + str(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada**: \n",
    "\n",
    "\n",
    "<table style=\"width:35%\">\n",
    "<tr>\n",
    "<td>   w   </td>\n",
    "<td> [[0.]\n",
    " [0.]\n",
    " [0.]\n",
    " [0.]\n",
    " [0.]\n",
    " [0.]] </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>   b   </td>\n",
    "<td> 0 </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2.3 \n",
    "#### Propagación hacia delante y hacia atrás\n",
    "\n",
    "Una vez los estimadores están inicializados, se pueden implementar los pasos de propagación hacia \"delante\" y hacia \"atrás\" para el aprendizaje automático. \n",
    "\n",
    "La propagación hacia delante consiste en calcular la función de activación sigmoide sobre la combinacón lineal de los patrones y los coeficientes inciales. \n",
    "\n",
    "Luego la propagación hacia atrás, o *retro-propagación*, es el paso más importante, donde utilizamos el gradiente de la función del error o de pérdida para actualizar los coeficientes. \n",
    "\n",
    "Este procedimiento se repite iterativamente replicando el procediemiento de descenso en la dirección del gradiente o *Gradient Descent* (GD).\n",
    "\n",
    "A continuación implemente la función `propaga()` que calcula la función de coste y su gradiente.\n",
    "\n",
    "**Ayuda**:\n",
    "\n",
    "Propagación hacia delante:\n",
    "- Se tiene $X$\n",
    "- Se calcula $A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$\n",
    "- Se calcula la función de coste/pérdida: $L = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$\n",
    "\n",
    "Para la retro-propagación, tenemos que calcular la derivada parcial de *L* con respecto a nuestros coeficientes $(w,b)$:  \n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{5}$$\n",
    "$$ \\frac{\\partial L}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{6}$$\n",
    "\n",
    "*Nota:* Para el cálculo de estas derivadas debemos hacer uso de la regla de la cadena. \n",
    "\n",
    "Esto es, dado $Z=w^T X + b$, se tiene que $$\\frac{\\partial L}{\\partial Z} = \\frac{\\partial L}{\\partial A} \\frac{\\partial A}{\\partial Z} = \\bigg(\\frac{-Y}{A}+\\frac{1-Y}{1-A}\\bigg) (A \\cdot (1-A)) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propaga(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Implemente la función de coste y su gradiente para la propagación\n",
    "    Input:\n",
    "    w: pesos, un arreglo numpy \n",
    "    b: sesgo, un escalar\n",
    "    X: datos de entrada\n",
    "    Y: vector de etiquetas \n",
    "    Output:\n",
    "    coste: coste negativo de log-verosimilitud para la regresión logística\n",
    "    dw: gradiente de la pérdida con respecto a w, con las mismas dimensiones que w\n",
    "    db: gradiente de la pérdida con respecto a b, con las mismas dimensiones que b\n",
    "    \n",
    "    (Sugerencia: utilice las funciones np.log(), np.dot()\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    # A: Activación (w traspuesto * X + b)\n",
    "    A = sigmoide(np.dot(w.T,X)+b)\n",
    "    \n",
    "    # Se calcula el costo en dos etapas: \n",
    "    # 1. El interior de la sumatoria\n",
    "    # 2. La sumatoria\n",
    "    logs = np.multiply(Y,np.log(A)) + np.multiply((1 - Y),np.log(1 - A))\n",
    "    coste = -1/m * np.sum(logs)    \n",
    "\n",
    "    # dw: gradiente de la pérdida con respecto a w, con las mismas dimensiones que w\n",
    "    # db: gradiente de la pérdida con respecto a b, con las mismas dimensiones que b\n",
    "    dw = 1/m * np.dot(X,(A-Y).T) \n",
    "    db = 1/m * np.sum(A-Y,keepdims=True)\n",
    "    \n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    coste = np.squeeze(coste)\n",
    "    assert(coste.shape == ())\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, coste\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dw = [[65.48251839]\n",
      " [29.66675568]]\n",
      "db = [[0.3489808]]\n",
      "coste = 9.752716367426284\n"
     ]
    }
   ],
   "source": [
    "w, b, X, Y = np.array([[0.1],[0.1]]), 0.5, np.array([[66.,99.,-33.],[32.,55.,-2.1]]), np.array([[0,0,1]])\n",
    "grads, coste = propaga(w, b, X, Y)\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "print (\"coste = \" + str(coste))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada**: \n",
    "\n",
    "<table style=\"width:50%\">\n",
    "<tr>\n",
    "<td>   dw   </td>\n",
    "<td> [[65.48251839]\n",
    " [29.66675568]]</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>   db   </td>\n",
    "<td> 0.348980796447886 </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>   cost   </td>\n",
    "<td> 9.752716367426284 </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Los parámetros obtenidos están de acuerdo a la salida espera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2.4 \n",
    "#### Optimización\n",
    "\n",
    "- Se tienen los parámetros inicializados.\n",
    "- También se tiene el código para calcular la función de coste y su gradiente.\n",
    "- Ahora se quieren actualizar los parámetros utilizando el GD.\n",
    "\n",
    "Escriba la función de optimización para aprender $w$ y $b$ minimizando la función de coste $L$. \n",
    "\n",
    "Para un parámetro $\\theta$, la regla de actualización es $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, donde $\\alpha$ es la tasa de aprendizaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimiza(w, b, X, Y, num_iter, tasa, print_cost):\n",
    "    \"\"\"\n",
    "    Esta función optimiza w y b implementando el algoritmo de GD\n",
    "    Entradas:\n",
    "    w: pesos, un arreglo numpy \n",
    "    b: sesgo, un escalar\n",
    "    X: datos de entrada\n",
    "    Y: vector de etiquetas \n",
    "    num_iter: número de iteracionespara el bucle de optimización\n",
    "    tasa: tasa de aprendizaje para la regla de actualización del GD\n",
    "    print_cost: True para imprimir la pérdida cada 100 iteraciones\n",
    "    Salidas:\n",
    "    params: diccionario con los pesos w y el sesgo b\n",
    "    grads: diccionario con los gradientes de los pesos y el sesgo con respecto a la función de pérdida\n",
    "    costes: lista de todos los costes calculados durante la optimización, usados para graficar la curva de aprendizaje.\n",
    "    \n",
    "    Sugerencia: puede escribir dos pasos e iterar sobre ellos:\n",
    "        1) Calcule el coste y el gradiente de los parámetros actuales. Use propaga().\n",
    "        2) Actualize los parámetros usando la regla del GD para w y b.\n",
    "    \"\"\"\n",
    "    \n",
    "    costes = []\n",
    "    \n",
    "    for i in range(num_iter):\n",
    "        \n",
    "        \n",
    "        # Computación del coste y el gradiente \n",
    "        grads, coste = propaga(w,b,X,Y)\n",
    "        \n",
    "        # Recupere las derivadas de grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # Actualize la regla \n",
    "        w = w - tasa * dw\n",
    "        b = b - tasa * db\n",
    "        \n",
    "        # Guarde los costes\n",
    "        if i % 100 == 0:\n",
    "            costes.append(coste)\n",
    "        \n",
    "        # Se muestra el coste cada 100 iteraciones de entrenamiento\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Coste tras la iteración %i: %f\" %(i, coste))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [[-0.07262234]\n",
      " [ 0.02112647]]\n",
      "b = [[0.49898149]]\n",
      "dw = [[1.42076721]\n",
      " [0.43496446]]\n",
      "db = [[-0.00782166]]\n"
     ]
    }
   ],
   "source": [
    "params, grads, costes = optimiza(w, b, X, Y, num_iter= 10, tasa = 0.001, print_cost = False)\n",
    "\n",
    "print (\"w = \" + str(params[\"w\"]))\n",
    "print (\"b = \" + str(params[\"b\"]))\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada**:  \n",
    "\n",
    "<table style=\"width:40%\">\n",
    "<tr>\n",
    "<td> w </td>\n",
    "<td>[[-0.07262234]\n",
    " [ 0.02112647]] </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td> b </td>\n",
    "<td> 0.49898148713402446 </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td> dw </td>\n",
    "<td> [[1.42076721]\n",
    " [0.43496446]] </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td> db </td>\n",
    "<td> -0.007821662502973652 </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2.5\n",
    "\n",
    "La función anterior aprende los parámetros w y b, que se pueden usar para predecir sobre el conjunto de datos X. \n",
    "\n",
    "Hay dos pasos para calcular las predicciones:\n",
    "\n",
    "1. Calcular $\\hat{Y} = A = \\sigma(w^T X + b)$\n",
    "\n",
    "2. Converir a 0 las entradas de $a$ (si la activación es <= 0.5) o 1 (si la activación es > 0.5), guarde las predicciones en un vector `Y_pred`.  \n",
    "\n",
    "Ahora implemente la función `pred()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(w, b, X):\n",
    "    '''\n",
    "    Prediga si una etiqueta es 0 o 1 usando los parámetros de regresión logística aprendidos (w, b)\n",
    "    Entrada:\n",
    "    w: pesos, un arreglo numpy \n",
    "    b: sesgo, un escalar\n",
    "    X: datos de entrada\n",
    "    Salida:\n",
    "    Y_pred: vector con todas las predicciones (0/1) para los ejemplos en X\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    Y_pred = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    # Se calcula \"A\" \n",
    "    A = sigmoide(np.dot(w.T,X)+b)\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        \n",
    "        # Convierta las probabilidades A[0,i] a predicciones p[0,i]\n",
    "        Y_pred[0,i] = round(A[0,i],0)\n",
    "    \n",
    "    assert(Y_pred.shape == (1, m))\n",
    "    \n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicciones = [[1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "w = np.array([[0.12],[0.23]])\n",
    "b = -0.09\n",
    "X = np.array([[3.1,-2.9,0.2],[1.9,1.8,-0.09]])\n",
    "print (\"predicciones = \" + str(pred(w, b, X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada**: \n",
    "\n",
    "<table style=\"width:40%\">\n",
    "<tr>\n",
    "<td> predicciones   </td>\n",
    "<td>[[ 1.  0.  0.]]  </td>  \n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2.6\n",
    "#### Combine todas las funciones \n",
    "\n",
    "Ahora juntemos todos los bloques que ha programado arriba.\n",
    "\n",
    "Implemente la función del modelo \"madre\". Use la siguiente notación:\n",
    "    - YP_pred para las predicciones sobre el conjunto de prueba\n",
    "    - YE_pred para las predicciones sobre el conjunto de entrenamiento\n",
    "    - w, costes, grads para las salidas de optimiza()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 61)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CP_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelo(CE_x, CP_x, CE_y, CP_y, num_iter, tasa, print_cost):\n",
    "    \"\"\"\n",
    "    Construye el modelo de regresión logística llamando las funciones implementadas anteriormente\n",
    "    Output:\n",
    "    d: diccionario con la información sobre el modelo.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Inicialice los parametros con ceros \n",
    "    w, b = inicializa_ceros(CE_x.shape[0])\n",
    "\n",
    "    # Descenso en la dirección del gradiente (GD) \n",
    "    params, grads, costes = optimiza(w, b, CE_x, CE_y, num_iter, tasa, print_cost)\n",
    "    \n",
    "    # Recupere los parámetros w y b del diccionario \"params\" ##\n",
    "    w = params[\"w\"]\n",
    "    b = params[\"b\"]\n",
    "    \n",
    "    # Prediga los ejemplos de prueba y entrenamiento (≈ 2 líneas de código)\n",
    "    YP_pred = pred(w, b, CP_x)\n",
    "    YE_pred = pred(w, b, CE_x)\n",
    "\n",
    "    # Imprima los errores de entrenamiento y prueba\n",
    "    print(\"Precisión de entrenamiento: {} %\".format(100 - np.mean(np.abs(YE_pred - CE_y)) * 100))\n",
    "    print(\"Precisión de prueba: {} %\".format(100 - np.mean(np.abs(YP_pred - CP_y)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"Costes\": costes,\n",
    "         \"Prediccion_prueba\": YP_pred, \n",
    "         \"Prediccion_entrenamiento\" : YE_pred, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"Tasa de aprendizaje\" : tasa,\n",
    "         \"Numero de iteraciones\": num_iter}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 2.7\n",
    "\n",
    "De qué dimensiones deben ser las matrices con los datos de entrada y de salida?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61, 600) (1, 600)\n"
     ]
    }
   ],
   "source": [
    "# Se hallan las dimensiones de las matrices con los datos de entrada y de salida\n",
    "CE_x2 = CE_x.T\n",
    "CP_x2 = CP_x.T\n",
    "CE_y2 = np.array(CE_y)[np.newaxis]\n",
    "CP_y2 = np.array(CP_y)[np.newaxis]\n",
    "\n",
    "print(CE_x2.shape, CE_y2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, ejecute la siguiente celda para entrenar el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 0.678157\n",
      "Coste tras la iteración 200: 0.677031\n",
      "Coste tras la iteración 300: 0.675938\n",
      "Coste tras la iteración 400: 0.674877\n",
      "Coste tras la iteración 500: 0.673846\n",
      "Coste tras la iteración 600: 0.672844\n",
      "Coste tras la iteración 700: 0.671871\n",
      "Coste tras la iteración 800: 0.670925\n",
      "Coste tras la iteración 900: 0.670005\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n"
     ]
    }
   ],
   "source": [
    "d = modelo(CE_x2, CP_x2, CE_y2, CP_y2, num_iter = 1000, tasa = 1e-6, print_cost = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada**: \n",
    "\n",
    "<table style=\"width:50%\"> \n",
    "<tr>\n",
    "<td> Coste tras la iteración 0   </td> \n",
    "<td> 0.693147 </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td> <center> $\\vdots$ </center> </td> \n",
    "<td> <center> $\\vdots$ </center> </td> \n",
    "</tr>  \n",
    "<tr>\n",
    "<td> Precisión de entrenamiento  </td> \n",
    "<td> 70.0 % </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td> Precisión de prueba </td> \n",
    "<td> 70.0 % </td>\n",
    "</tr>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La precisión de entrenamiento es muy similar a la que conseguimos mediante la regresion logistica. También podemos observar que el error de prueba es igual al de entrenamiento. Este resultado sugiere que el modelo aprende segun entrenamiento, y generaliza de igual forma sobre los observaciones nuevas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grafiquemos la función de pérdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gráfica de la curva de aprendizaje (con costes)\n",
    "costes = np.squeeze(d['Costes'])\n",
    "plt.plot(costes)\n",
    "plt.ylabel('coste')\n",
    "plt.xlabel('iteraciones (en cientos)')\n",
    "plt.title(\"Tasa de aprendizaje =\" + str(d[\"Tasa de aprendizaje\"]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretación**:\n",
    "Se puede ver el coste decreciendo, demostrando que los parámetros están siendo aprendidos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya tenemos un primer modelo de clasificación. Ahora examinemos distintos valores para la tasa de aprendizaje $\\alpha$. \n",
    "\n",
    "#### Selección de la tasa de aprendizaje ####\n",
    "\n",
    "Para que el método del GD funcione de manera adecuada, se debe elegir la tasa de aprendiazaje de manera acertada. Esta tasa $\\alpha$  determina qué tan rápido se actualizan los parámetros. Si la tasa es muy grande se puede \"sobrepasar\" el valor óptimo. Y de manera similar, si es muy pequeña se van a necesitar muchas iteraciones para converger a los mejores valores. Por ello la importancia de tener una tasa de aprendizaje bien definida.  \n",
    "\n",
    "Ahora, comparemos la curva de aprendizaje de nuestro modelo con distintas elecciones para $\\alpha$. Ejecute el código abajo. También puede intentar con valores distintos a los tres que estamos utilizando abajo para `tasas` y analize los resultados.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La tasa de aprendizaje es: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-bd0ffb3a48e4>:9: RuntimeWarning: overflow encountered in exp\n",
      "  s = 1/(1+np.exp(-z))\n",
      "<ipython-input-8-07deeef0e3d8>:24: RuntimeWarning: divide by zero encountered in log\n",
      "  logs = np.multiply(Y,np.log(A)) + np.multiply((1 - Y),np.log(1 - A))\n",
      "<ipython-input-8-07deeef0e3d8>:24: RuntimeWarning: invalid value encountered in multiply\n",
      "  logs = np.multiply(Y,np.log(A)) + np.multiply((1 - Y),np.log(1 - A))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión de entrenamiento: 33.0 %\n",
      "Precisión de prueba: 32.75 %\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "La tasa de aprendizaje es: 1.04e-06\n",
      "Precisión de entrenamiento: 70.16666666666666 %\n",
      "Precisión de prueba: 72.5 %\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "La tasa de aprendizaje es: 1e-06\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "La tasa de aprendizaje es: 1e-10\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "La tasa de aprendizaje es: 2e-20\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tasas = [1e-4, 1.04e-6, 1e-6, 1e-10, 2e-20]\n",
    "modelos = {}\n",
    "for i in tasas:\n",
    "    print (\"La tasa de aprendizaje es: \" + str(i))\n",
    "    modelos[str(i)] = modelo(CE_x2, CP_x2, CE_y2, CP_y2, num_iter = 2000, tasa = i, print_cost = False)\n",
    "    print ('\\n' + \"-------------------------------------------------------\" + '\\n')\n",
    "\n",
    "for i in tasas:\n",
    "    plt.plot(np.squeeze(modelos[str(i)][\"Costes\"]), label= str(modelos[str(i)][\"Tasa de aprendizaje\"]))\n",
    "\n",
    "plt.ylabel('coste')\n",
    "plt.xlabel('iteraciones (en cientos)')\n",
    "\n",
    "legend = plt.legend(loc='upper center', shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comentarios acerca de la tasa de aprendizaje\n",
    "\n",
    "Los pesos de cada neurona para el algoritmo GD **gradient descent**, donde el error de cada neurona está relacionado con su peso. La regla para actualizar el parámetro es: $ \\theta = \\theta - \\alpha \\text{ } d\\theta$ donde $\\alpha$ es la tasa de aprendizaje $\\theta$.\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/albahnsen/PracticalMachineLearningClass/master/notebooks/images/updateParameters.png'>\n",
    "\n",
    "Con una tasa de muy baja (en nuestro caso $1*10^{20}$), se requieren muchas iteraciones antes de alcanzar el punto mínimo; mientras que una tasa de aprendizaje muy alta (en nuestro caso $1*10^{4}$), causas actualizaciones muy drásticas en los pesos lo que conlleva a comportamientos erráticos .  En contraposición una buena tasa de aprendizaje (en nuestro caso $1*10^{6}$) optimiza el tiempo de cómputo, así como la precisión de la red neuronal. Imágenes tomadas de <a href=\"https://raw.githubusercontent.com/albahnsen/PracticalMachineLearningClass/master/notebooks/images/updateParameters.png\">Repositorio git hub de Alejandro Correa</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 2.8\n",
    "\n",
    "Analice los resultados, con cuál tasa de aprendizaje intentaría mejorar el desempeño del modelo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R/ Valores entre $1.04*10^6$ y $1*10^6$ se podría encontrar el máximo absoluto. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparacion con la implementación tradicional de regresión logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación ajustamos el modelo logístico y lo probamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "logT = LogisticRegression(penalty='none', max_iter=1500)\n",
    "logT.fit(CE_x, CE_y)\n",
    "y_tr = logT.predict(CE_x)\n",
    "y_pred = logT.predict(CP_x)\n",
    "logT_coef = logT.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examinemos los coeficientes del modelo de la neurona sigmoide y su desviación con respecto a la estimación tradicional de regresion logistica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<i>Table length=61</i>\n",
       "<table id=\"table4906718880\" class=\"table-striped table-bordered table-condensed\">\n",
       "<thead><tr><th>Regresion logistica [1]</th><th>Neurona sigmoide [1]</th><th>Diferencia [600]</th></tr></thead>\n",
       "<thead><tr><th>float64</th><th>float64</th><th>int64</th></tr></thead>\n",
       "<tr><td>0.0228025923589621</td><td>-6.293083905403859e-05</td><td>18 .. 15</td></tr>\n",
       "<tr><td>7.431426568141854e-05</td><td>-1.631136907400603e-05</td><td>12976 .. 3029</td></tr>\n",
       "<tr><td>0.24834399085865466</td><td>-0.00015135061139790666</td><td>3 .. 2</td></tr>\n",
       "<tr><td>0.05379887762733803</td><td>-0.00016794413611101247</td><td>4 .. 2</td></tr>\n",
       "<tr><td>-0.009715877848817596</td><td>-0.0023681544427456127</td><td>38 .. 33</td></tr>\n",
       "<tr><td>0.08401499725551834</td><td>-8.574184624564529e-05</td><td>1 .. 1</td></tr>\n",
       "<tr><td>0.02925479496657787</td><td>-6.586234732681293e-05</td><td>1 .. 1</td></tr>\n",
       "<tr><td>0.5398583553972339</td><td>2.9167410750735905e-05</td><td>0 .. 0</td></tr>\n",
       "<tr><td>0.1388022989118418</td><td>8.429711287300682e-06</td><td>1 .. 0</td></tr>\n",
       "<tr><td>-0.22214775368754958</td><td>-5.488449210699085e-06</td><td>0 .. 0</td></tr>\n",
       "<tr><td>...</td><td>...</td><td>...</td></tr>\n",
       "<tr><td>-0.45028936612338194</td><td>-7.701646645973134e-05</td><td>0 .. 1</td></tr>\n",
       "<tr><td>-0.3720941124917211</td><td>3.986121808875351e-06</td><td>1 .. 0</td></tr>\n",
       "<tr><td>-0.015344100246621794</td><td>1.991680499858925e-06</td><td>0 .. 0</td></tr>\n",
       "<tr><td>-0.07946983871933141</td><td>-1.5375843769924963e-05</td><td>0 .. 0</td></tr>\n",
       "<tr><td>-0.0977936532881251</td><td>-4.5352990748194025e-05</td><td>0 .. 1</td></tr>\n",
       "<tr><td>-0.2588428937327082</td><td>-7.254015435877615e-07</td><td>1 .. 0</td></tr>\n",
       "<tr><td>-0.09421005575489608</td><td>-2.70263683647599e-05</td><td>0 .. 1</td></tr>\n",
       "<tr><td>-0.357240430182823</td><td>-3.2436187197087996e-05</td><td>1 .. 0</td></tr>\n",
       "<tr><td>0.1821101050272629</td><td>-4.6792409931105244e-05</td><td>1 .. 1</td></tr>\n",
       "<tr><td>-0.6335605909571764</td><td>-1.2670145630742544e-05</td><td>0 .. 0</td></tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Table length=61>\n",
       "Regresion logistica [1]   Neurona sigmoide [1]  Diferencia [600]\n",
       "        float64                 float64              int64      \n",
       "----------------------- ----------------------- ----------------\n",
       "     0.0228025923589621  -6.293083905403859e-05         18 .. 15\n",
       "  7.431426568141854e-05  -1.631136907400603e-05    12976 .. 3029\n",
       "    0.24834399085865466 -0.00015135061139790666           3 .. 2\n",
       "    0.05379887762733803 -0.00016794413611101247           4 .. 2\n",
       "  -0.009715877848817596  -0.0023681544427456127         38 .. 33\n",
       "    0.08401499725551834  -8.574184624564529e-05           1 .. 1\n",
       "    0.02925479496657787  -6.586234732681293e-05           1 .. 1\n",
       "     0.5398583553972339  2.9167410750735905e-05           0 .. 0\n",
       "     0.1388022989118418   8.429711287300682e-06           1 .. 0\n",
       "   -0.22214775368754958  -5.488449210699085e-06           0 .. 0\n",
       "                    ...                     ...              ...\n",
       "   -0.45028936612338194  -7.701646645973134e-05           0 .. 1\n",
       "    -0.3720941124917211   3.986121808875351e-06           1 .. 0\n",
       "  -0.015344100246621794   1.991680499858925e-06           0 .. 0\n",
       "   -0.07946983871933141 -1.5375843769924963e-05           0 .. 0\n",
       "    -0.0977936532881251 -4.5352990748194025e-05           0 .. 1\n",
       "    -0.2588428937327082  -7.254015435877615e-07           1 .. 0\n",
       "   -0.09421005575489608   -2.70263683647599e-05           0 .. 1\n",
       "     -0.357240430182823 -3.2436187197087996e-05           1 .. 0\n",
       "     0.1821101050272629 -4.6792409931105244e-05           1 .. 1\n",
       "    -0.6335605909571764 -1.2670145630742544e-05           0 .. 0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from astropy.table import QTable, Table, Column\n",
    "\n",
    "Tabla =  Table([logT_coef.T, d['w'], CE_x.T], names=(\"Regresion logistica\", \"Neurona sigmoide\", \"Diferencia\"))\n",
    "Tabla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 3.1\n",
    "\n",
    "Qué puede observar en esta comparativa?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se nota diferencia entre las unidades de la estimación. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos la exactitud de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La neurona sigmoide tiene una exactitud de entrenamiento: 0.7 y de validacion: 0.7\n",
      "La regresion tradicional tiene una exactitud de entrenamiento: 0.77 y de validacion: 0.76\n"
     ]
    }
   ],
   "source": [
    "print(\"La neurona sigmoide tiene una exactitud de entrenamiento: \" \n",
    "      +str(float((d['Prediccion_entrenamiento'] == CE_y2).mean())) +\" y de validacion: \" +str(float((d['Prediccion_prueba'] == CP_y2).mean())))\n",
    "print(\"La regresion tradicional tiene una exactitud de entrenamiento: \" \n",
    "      +str(float((y_tr == CE_y).mean())) +\" y de validacion: \" +str(float((y_pred == CP_y).mean())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio  3.2\n",
    "\n",
    "Ahora puede desarrollar su propio código intentando mejorar los resultados obtenidos. \n",
    "\n",
    "Intente sobrepasar los resultados de la regresion logistica tradicional. Optimice la tasa de aprendizaje, el número de iteraciones o (bono) investigue y cambie la manera en la cual inicializamos los coeficientes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** tasa = 10 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: nan\n",
      "Coste tras la iteración 200: nan\n",
      "Coste tras la iteración 300: nan\n",
      "Coste tras la iteración 400: nan\n",
      "Coste tras la iteración 500: nan\n",
      "Coste tras la iteración 600: nan\n",
      "Coste tras la iteración 700: nan\n",
      "Coste tras la iteración 800: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-bd0ffb3a48e4>:9: RuntimeWarning: overflow encountered in exp\n",
      "  s = 1/(1+np.exp(-z))\n",
      "<ipython-input-8-07deeef0e3d8>:24: RuntimeWarning: divide by zero encountered in log\n",
      "  logs = np.multiply(Y,np.log(A)) + np.multiply((1 - Y),np.log(1 - A))\n",
      "<ipython-input-8-07deeef0e3d8>:24: RuntimeWarning: invalid value encountered in multiply\n",
      "  logs = np.multiply(Y,np.log(A)) + np.multiply((1 - Y),np.log(1 - A))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coste tras la iteración 900: nan\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 20 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: nan\n",
      "Coste tras la iteración 200: inf\n",
      "Coste tras la iteración 300: nan\n",
      "Coste tras la iteración 400: nan\n",
      "Coste tras la iteración 500: nan\n",
      "Coste tras la iteración 600: nan\n",
      "Coste tras la iteración 700: nan\n",
      "Coste tras la iteración 800: nan\n",
      "Coste tras la iteración 900: nan\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 30 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 32.965270\n",
      "Coste tras la iteración 200: inf\n",
      "Coste tras la iteración 300: nan\n",
      "Coste tras la iteración 400: 32.749237\n",
      "Coste tras la iteración 500: inf\n",
      "Coste tras la iteración 600: nan\n",
      "Coste tras la iteración 700: 32.515398\n",
      "Coste tras la iteración 800: nan\n",
      "Coste tras la iteración 900: nan\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 40 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 24.148744\n",
      "Coste tras la iteración 200: inf\n",
      "Coste tras la iteración 300: nan\n",
      "Coste tras la iteración 400: 23.989958\n",
      "Coste tras la iteración 500: inf\n",
      "Coste tras la iteración 600: nan\n",
      "Coste tras la iteración 700: 23.820505\n",
      "Coste tras la iteración 800: inf\n",
      "Coste tras la iteración 900: nan\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 50 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 18.858754\n",
      "Coste tras la iteración 200: inf\n",
      "Coste tras la iteración 300: 39.982660\n",
      "Coste tras la iteración 400: 18.733316\n",
      "Coste tras la iteración 500: inf\n",
      "Coste tras la iteración 600: 39.852418\n",
      "Coste tras la iteración 700: 18.600816\n",
      "Coste tras la iteración 800: inf\n",
      "Coste tras la iteración 900: 39.715575\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 60 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 15.332199\n",
      "Coste tras la iteración 200: 4.846013\n",
      "Coste tras la iteración 300: 32.935902\n",
      "Coste tras la iteración 400: 15.228542\n",
      "Coste tras la iteración 500: inf\n",
      "Coste tras la iteración 600: 32.828854\n",
      "Coste tras la iteración 700: 15.119878\n",
      "Coste tras la iteración 800: inf\n",
      "Coste tras la iteración 900: 32.717055\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 70 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 12.813613\n",
      "Coste tras la iteración 200: 4.825546\n",
      "Coste tras la iteración 300: 27.902688\n",
      "Coste tras la iteración 400: 12.725260\n",
      "Coste tras la iteración 500: inf\n",
      "Coste tras la iteración 600: 27.811823\n",
      "Coste tras la iteración 700: 12.633181\n",
      "Coste tras la iteración 800: inf\n",
      "Coste tras la iteración 900: 27.717387\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 80 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 10.925480\n",
      "Coste tras la iteración 200: 4.808059\n",
      "Coste tras la iteración 300: 24.128394\n",
      "Coste tras la iteración 400: 10.848432\n",
      "Coste tras la iteración 500: inf\n",
      "Coste tras la iteración 600: 24.049422\n",
      "Coste tras la iteración 700: 10.768510\n",
      "Coste tras la iteración 800: inf\n",
      "Coste tras la iteración 900: 23.967676\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 90 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 9.458334\n",
      "Coste tras la iteración 200: 4.790793\n",
      "Coste tras la iteración 300: 21.194019\n",
      "Coste tras la iteración 400: 9.389942\n",
      "Coste tras la iteración 500: inf\n",
      "Coste tras la iteración 600: 21.124118\n",
      "Coste tras la iteración 700: 9.319273\n",
      "Coste tras la iteración 800: inf\n",
      "Coste tras la iteración 900: 21.052010\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 100 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 8.286787\n",
      "Coste tras la iteración 200: 4.771513\n",
      "Coste tras la iteración 300: 18.848433\n",
      "Coste tras la iteración 400: 8.225204\n",
      "Coste tras la iteración 500: inf\n",
      "Coste tras la iteración 600: 18.785651\n",
      "Coste tras la iteración 700: 8.161778\n",
      "Coste tras la iteración 800: inf\n",
      "Coste tras la iteración 900: 18.721078\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 110 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 7.331348\n",
      "Coste tras la iteración 200: 4.747081\n",
      "Coste tras la iteración 300: 16.932118\n",
      "Coste tras la iteración 400: 7.275238\n",
      "Coste tras la iteración 500: inf\n",
      "Coste tras la iteración 600: 16.875047\n",
      "Coste tras la iteración 700: 7.217612\n",
      "Coste tras la iteración 800: inf\n",
      "Coste tras la iteración 900: 16.816506\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 120 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 6.539298\n",
      "Coste tras la iteración 200: 4.717773\n",
      "Coste tras la iteración 300: 15.338995\n",
      "Coste tras la iteración 400: 6.487673\n",
      "Coste tras la iteración 500: 4.812546\n",
      "Coste tras la iteración 600: 15.286601\n",
      "Coste tras la iteración 700: 6.434787\n",
      "Coste tras la iteración 800: inf\n",
      "Coste tras la iteración 900: 15.232993\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 130 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 5.874354\n",
      "Coste tras la iteración 200: 4.679924\n",
      "Coste tras la iteración 300: 13.995835\n",
      "Coste tras la iteración 400: 5.826476\n",
      "Coste tras la iteración 500: 4.768636\n",
      "Coste tras la iteración 600: 13.947353\n",
      "Coste tras la iteración 700: 5.777545\n",
      "Coste tras la iteración 800: inf\n",
      "Coste tras la iteración 900: 13.897864\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 140 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 5.310704\n",
      "Coste tras la iteración 200: 4.632936\n",
      "Coste tras la iteración 300: 12.850442\n",
      "Coste tras la iteración 400: 5.266024\n",
      "Coste tras la iteración 500: 4.715222\n",
      "Coste tras la iteración 600: 12.805307\n",
      "Coste tras la iteración 700: 5.220470\n",
      "Coste tras la iteración 800: 4.799202\n",
      "Coste tras la iteración 900: 12.759339\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 150 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 4.829386\n",
      "Coste tras la iteración 200: 4.575112\n",
      "Coste tras la iteración 300: 11.864509\n",
      "Coste tras la iteración 400: 4.787504\n",
      "Coste tras la iteración 500: 4.652612\n",
      "Coste tras la iteración 600: 11.822311\n",
      "Coste tras la iteración 700: 4.744907\n",
      "Coste tras la iteración 800: 4.731881\n",
      "Coste tras la iteración 900: 11.779428\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 160 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 4.416001\n",
      "Coste tras la iteración 200: 4.506726\n",
      "Coste tras la iteración 300: 11.009137\n",
      "Coste tras la iteración 400: 4.376633\n",
      "Coste tras la iteración 500: 4.579546\n",
      "Coste tras la iteración 600: 10.969582\n",
      "Coste tras la iteración 700: 4.336691\n",
      "Coste tras la iteración 800: 4.653690\n",
      "Coste tras la iteración 900: 10.929471\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 170 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 4.059256\n",
      "Coste tras la iteración 200: 4.427643\n",
      "Coste tras la iteración 300: 10.261948\n",
      "Coste tras la iteración 400: 4.022201\n",
      "Coste tras la iteración 500: 4.496314\n",
      "Coste tras la iteración 600: 10.224823\n",
      "Coste tras la iteración 700: 3.984696\n",
      "Coste tras la iteración 800: 4.565760\n",
      "Coste tras la iteración 900: 10.187255\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 180 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 3.750034\n",
      "Coste tras la iteración 200: 4.339142\n",
      "Coste tras la iteración 300: 9.605205\n",
      "Coste tras la iteración 400: 3.715144\n",
      "Coste tras la iteración 500: 4.403700\n",
      "Coste tras la iteración 600: 9.570351\n",
      "Coste tras la iteración 700: 3.679911\n",
      "Coste tras la iteración 800: 4.468810\n",
      "Coste tras la iteración 900: 9.535150\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 190 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 3.480821\n",
      "Coste tras la iteración 200: 4.242745\n",
      "Coste tras la iteración 300: 9.024588\n",
      "Coste tras la iteración 400: 3.447979\n",
      "Coste tras la iteración 500: 4.303339\n",
      "Coste tras la iteración 600: 8.991873\n",
      "Coste tras la iteración 700: 3.414884\n",
      "Coste tras la iteración 800: 4.364384\n",
      "Coste tras la iteración 900: 8.958897\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 200 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 3.245351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coste tras la iteración 200: 4.140236\n",
      "Coste tras la iteración 300: 8.508383\n",
      "Coste tras la iteración 400: 3.214449\n",
      "Coste tras la iteración 500: 4.197113\n",
      "Coste tras la iteración 600: 8.477691\n",
      "Coste tras la iteración 700: 3.183371\n",
      "Coste tras la iteración 800: 4.254303\n",
      "Coste tras la iteración 900: 8.446810\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 210 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 3.038375\n",
      "Coste tras la iteración 200: 4.033480\n",
      "Coste tras la iteración 300: 8.046935\n",
      "Coste tras la iteración 400: 3.009310\n",
      "Coste tras la iteración 500: 4.086816\n",
      "Coste tras la iteración 600: 8.018154\n",
      "Coste tras la iteración 700: 2.980134\n",
      "Coste tras la iteración 800: 4.140308\n",
      "Coste tras la iteración 900: 7.989244\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 220 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 2.855501\n",
      "Coste tras la iteración 200: 3.924227\n",
      "Coste tras la iteración 300: 7.632249\n",
      "Coste tras la iteración 400: 2.828170\n",
      "Coste tras la iteración 500: 3.974194\n",
      "Coste tras la iteración 600: 7.605267\n",
      "Coste tras la iteración 700: 2.800781\n",
      "Coste tras la iteración 800: 4.024212\n",
      "Coste tras la iteración 900: 7.578207\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 230 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 2.693063\n",
      "Coste tras la iteración 200: 3.813994\n",
      "Coste tras la iteración 300: 7.257685\n",
      "Coste tras la iteración 400: 2.667362\n",
      "Coste tras la iteración 500: 3.860787\n",
      "Coste tras la iteración 600: 7.232392\n",
      "Coste tras la iteración 700: 2.641646\n",
      "Coste tras la iteración 800: 3.907538\n",
      "Coste tras la iteración 900: 7.207063\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 240 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 2.548015\n",
      "Coste tras la iteración 200: 3.704036\n",
      "Coste tras la iteración 300: 6.917720\n",
      "Coste tras la iteración 400: 2.523839\n",
      "Coste tras la iteración 500: 3.747848\n",
      "Coste tras la iteración 600: 6.894006\n",
      "Coste tras la iteración 700: 2.499685\n",
      "Coste tras la iteración 800: 3.791546\n",
      "Coste tras la iteración 900: 6.870290\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 250 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 2.417823\n",
      "Coste tras la iteración 200: 3.595340\n",
      "Coste tras la iteración 300: 6.607743\n",
      "Coste tras la iteración 400: 2.395071\n",
      "Coste tras la iteración 500: 3.636365\n",
      "Coste tras la iteración 600: 6.585503\n",
      "Coste tras la iteración 700: 2.372369\n",
      "Coste tras la iteración 800: 3.677217\n",
      "Coste tras la iteración 900: 6.563287\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 260 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 2.300385\n",
      "Coste tras la iteración 200: 3.488656\n",
      "Coste tras la iteración 300: 6.323896\n",
      "Coste tras la iteración 400: 2.278958\n",
      "Coste tras la iteración 500: 3.527083\n",
      "Coste tras la iteración 600: 6.303027\n",
      "Coste tras la iteración 700: 2.257604\n",
      "Coste tras la iteración 800: 3.565289\n",
      "Coste tras la iteración 900: 6.282204\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 270 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 2.193951\n",
      "Coste tras la iteración 200: 3.384525\n",
      "Coste tras la iteración 300: 6.062937\n",
      "Coste tras la iteración 400: 2.173756\n",
      "Coste tras la iteración 500: 3.420534\n",
      "Coste tras la iteración 600: 6.043342\n",
      "Coste tras la iteración 700: 2.153653\n",
      "Coste tras la iteración 800: 3.456286\n",
      "Coste tras la iteración 900: 6.023810\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 280 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 2.097065\n",
      "Coste tras la iteración 200: 3.283320\n",
      "Coste tras la iteración 300: 5.822126\n",
      "Coste tras la iteración 400: 2.078012\n",
      "Coste tras la iteración 500: 3.317083\n",
      "Coste tras la iteración 600: 5.803715\n",
      "Coste tras la iteración 700: 2.059067\n",
      "Coste tras la iteración 800: 3.350560\n",
      "Coste tras la iteración 900: 5.785379\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 290 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 2.008504\n",
      "Coste tras la iteración 200: 3.185281\n",
      "Coste tras la iteración 300: 5.599142\n",
      "Coste tras la iteración 400: 1.990512\n",
      "Coste tras la iteración 500: 3.216958\n",
      "Coste tras la iteración 600: 5.581830\n",
      "Coste tras la iteración 700: 1.972639\n",
      "Coste tras la iteración 800: 3.248327\n",
      "Coste tras la iteración 900: 5.564602\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 300 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 1.927244\n",
      "Coste tras la iteración 200: 3.090542\n",
      "Coste tras la iteración 300: 5.392004\n",
      "Coste tras la iteración 400: 1.910236\n",
      "Coste tras la iteración 500: 3.120284\n",
      "Coste tras la iteración 600: 5.375712\n",
      "Coste tras la iteración 700: 1.893355\n",
      "Coste tras la iteración 800: 3.149701\n",
      "Coste tras la iteración 900: 5.359512\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 310 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 1.852418\n",
      "Coste tras la iteración 200: 2.999162\n",
      "Coste tras la iteración 300: 5.199014\n",
      "Coste tras la iteración 400: 1.836322\n",
      "Coste tras la iteración 500: 3.027107\n",
      "Coste tras la iteración 600: 5.183671\n",
      "Coste tras la iteración 700: 1.820361\n",
      "Coste tras la iteración 800: 3.054716\n",
      "Coste tras la iteración 900: 5.168423\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 320 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 1.783290\n",
      "Coste tras la iteración 200: 2.911140\n",
      "Coste tras la iteración 300: 5.018710\n",
      "Coste tras la iteración 400: 1.768041\n",
      "Coste tras la iteración 500: 2.937415\n",
      "Coste tras la iteración 600: 5.004248\n",
      "Coste tras la iteración 700: 1.752932\n",
      "Coste tras la iteración 800: 2.963347\n",
      "Coste tras la iteración 900: 4.989885\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 330 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 1.719232\n",
      "Coste tras la iteración 200: 2.826431\n",
      "Coste tras la iteración 300: 4.849826\n",
      "Coste tras la iteración 400: 1.704770\n",
      "Coste tras la iteración 500: 2.851155\n",
      "Coste tras la iteración 600: 4.836184\n",
      "Coste tras la iteración 700: 1.690452\n",
      "Coste tras la iteración 800: 2.875531\n",
      "Coste tras la iteración 900: 4.822643\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 340 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 1.659704\n",
      "Coste tras la iteración 200: 2.744963\n",
      "Coste tras la iteración 300: 4.691259\n",
      "Coste tras la iteración 400: 1.645974\n",
      "Coste tras la iteración 500: 2.768244\n",
      "Coste tras la iteración 600: 4.678382\n",
      "Coste tras la iteración 700: 1.632390\n",
      "Coste tras la iteración 800: 2.791173\n",
      "Coste tras la iteración 900: 4.665605\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 350 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 1.604243\n",
      "Coste tras la iteración 200: 2.666641\n",
      "Coste tras la iteración 300: 4.542045\n",
      "Coste tras la iteración 400: 1.591194\n",
      "Coste tras la iteración 500: 2.688577\n",
      "Coste tras la iteración 600: 4.529881\n",
      "Coste tras la iteración 700: 1.578292\n",
      "Coste tras la iteración 800: 2.710162\n",
      "Coste tras la iteración 900: 4.517817\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 360 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 1.552445\n",
      "Coste tras la iteración 200: 2.591356\n",
      "Coste tras la iteración 300: 4.401340\n",
      "Coste tras la iteración 400: 1.540031\n",
      "Coste tras la iteración 500: 2.612039\n",
      "Coste tras la iteración 600: 4.389842\n",
      "Coste tras la iteración 700: 1.527764\n",
      "Coste tras la iteración 800: 2.632372\n",
      "Coste tras la iteración 900: 4.378442\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 370 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 1.503960\n",
      "Coste tras la iteración 200: 2.518992\n",
      "Coste tras la iteración 300: 4.268395\n",
      "Coste tras la iteración 400: 1.492137\n",
      "Coste tras la iteración 500: 2.538504\n",
      "Coste tras la iteración 600: 4.257520\n",
      "Coste tras la iteración 700: 1.480462\n",
      "Coste tras la iteración 800: 2.557670\n",
      "Coste tras la iteración 900: 4.246742\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 380 ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 1.458479\n",
      "Coste tras la iteración 200: 2.449427\n",
      "Coste tras la iteración 300: 4.142550\n",
      "Coste tras la iteración 400: 1.447210\n",
      "Coste tras la iteración 500: 2.467845\n",
      "Coste tras la iteración 600: 4.132259\n",
      "Coste tras la iteración 700: 1.436086\n",
      "Coste tras la iteración 800: 2.485920\n",
      "Coste tras la iteración 900: 4.122062\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 390 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 1.415733\n",
      "Coste tras la iteración 200: 2.382538\n",
      "Coste tras la iteración 300: 4.023217\n",
      "Coste tras la iteración 400: 1.404981\n",
      "Coste tras la iteración 500: 2.399932\n",
      "Coste tras la iteración 600: 4.013474\n",
      "Coste tras la iteración 700: 1.394372\n",
      "Coste tras la iteración 800: 2.416988\n",
      "Coste tras la iteración 900: 4.003821\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 400 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 1.375484\n",
      "Coste tras la iteración 200: 2.318202\n",
      "Coste tras la iteración 300: 3.909870\n",
      "Coste tras la iteración 400: 1.365215\n",
      "Coste tras la iteración 500: 2.334636\n",
      "Coste tras la iteración 600: 3.900642\n",
      "Coste tras la iteración 700: 1.355089\n",
      "Coste tras la iteración 800: 2.350736\n",
      "Coste tras la iteración 900: 3.891501\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 410 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 1.337518\n",
      "Coste tras la iteración 200: 2.256298\n",
      "Coste tras la iteración 300: 3.802040\n",
      "Coste tras la iteración 400: 1.327704\n",
      "Coste tras la iteración 500: 2.271831\n",
      "Coste tras la iteración 600: 3.793297\n",
      "Coste tras la iteración 700: 1.318029\n",
      "Coste tras la iteración 800: 2.287035\n",
      "Coste tras la iteración 900: 3.784636\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 420 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 1.301649\n",
      "Coste tras la iteración 200: 2.196708\n",
      "Coste tras la iteración 300: 3.699303\n",
      "Coste tras la iteración 400: 1.292260\n",
      "Coste tras la iteración 500: 2.211393\n",
      "Coste tras la iteración 600: 3.691017\n",
      "Coste tras la iteración 700: 1.283010\n",
      "Coste tras la iteración 800: 2.225755\n",
      "Coste tras la iteración 900: 3.682809\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 430 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 1.267707\n",
      "Coste tras la iteración 200: 2.139316\n",
      "Coste tras la iteración 300: 3.601278\n",
      "Coste tras la iteración 400: 1.258719\n",
      "Coste tras la iteración 500: 2.153203\n",
      "Coste tras la iteración 600: 3.593424\n",
      "Coste tras la iteración 700: 1.249866\n",
      "Coste tras la iteración 800: 2.166772\n",
      "Coste tras la iteración 900: 3.585643\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 440 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 1.235541\n",
      "Coste tras la iteración 200: 2.084011\n",
      "Coste tras la iteración 300: 3.507620\n",
      "Coste tras la iteración 400: 1.226930\n",
      "Coste tras la iteración 500: 2.097144\n",
      "Coste tras la iteración 600: 3.500173\n",
      "Coste tras la iteración 700: 1.218452\n",
      "Coste tras la iteración 800: 2.109966\n",
      "Coste tras la iteración 900: 3.492797\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 450 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 1.205014\n",
      "Coste tras la iteración 200: 2.030686\n",
      "Coste tras la iteración 300: 3.418013\n",
      "Coste tras la iteración 400: 1.196758\n",
      "Coste tras la iteración 500: 2.043107\n",
      "Coste tras la iteración 600: 3.410954\n",
      "Coste tras la iteración 700: 1.188633\n",
      "Coste tras la iteración 800: 2.055224\n",
      "Coste tras la iteración 900: 3.403960\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 460 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 1.176004\n",
      "Coste tras la iteración 200: 1.979236\n",
      "Coste tras la iteración 300: 3.332173\n",
      "Coste tras la iteración 400: 1.168083\n",
      "Coste tras la iteración 500: 1.990984\n",
      "Coste tras la iteración 600: 3.325482\n",
      "Coste tras la iteración 700: 1.160290\n",
      "Coste tras la iteración 800: 2.002433\n",
      "Coste tras la iteración 900: 3.318850\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 470 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 1.148397\n",
      "Coste tras la iteración 200: 1.929562\n",
      "Coste tras la iteración 300: 3.249837\n",
      "Coste tras la iteración 400: 1.140793\n",
      "Coste tras la iteración 500: 1.940671\n",
      "Coste tras la iteración 600: 3.243496\n",
      "Coste tras la iteración 700: 1.133314\n",
      "Coste tras la iteración 800: 1.951488\n",
      "Coste tras la iteración 900: 3.237209\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 480 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 1.122091\n",
      "Coste tras la iteración 200: 1.881567\n",
      "Coste tras la iteración 300: 3.170765\n",
      "Coste tras la iteración 400: 1.114788\n",
      "Coste tras la iteración 500: 1.892071\n",
      "Coste tras la iteración 600: 3.164757\n",
      "Coste tras la iteración 700: 1.107605\n",
      "Coste tras la iteración 800: 1.902287\n",
      "Coste tras la iteración 900: 3.158798\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 490 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 1.096994\n",
      "Coste tras la iteración 200: 1.835158\n",
      "Coste tras la iteración 300: 3.094736\n",
      "Coste tras la iteración 400: 1.089974\n",
      "Coste tras la iteración 500: 1.845086\n",
      "Coste tras la iteración 600: 3.089046\n",
      "Coste tras la iteración 700: 1.083073\n",
      "Coste tras la iteración 800: 1.854731\n",
      "Coste tras la iteración 900: 3.083400\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 500 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 1.073018\n",
      "Coste tras la iteración 200: 1.790246\n",
      "Coste tras la iteración 300: 3.021544\n",
      "Coste tras la iteración 400: 1.066268\n",
      "Coste tras la iteración 500: 1.799623\n",
      "Coste tras la iteración 600: 3.016160\n",
      "Coste tras la iteración 700: 1.059634\n",
      "Coste tras la iteración 800: 1.808725\n",
      "Coste tras la iteración 900: 3.010812\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 510 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 1.050084\n",
      "Coste tras la iteración 200: 1.746741\n",
      "Coste tras la iteración 300: 2.951000\n",
      "Coste tras la iteración 400: 1.043591\n",
      "Coste tras la iteración 500: 1.755594\n",
      "Coste tras la iteración 600: 2.945907\n",
      "Coste tras la iteración 700: 1.037209\n",
      "Coste tras la iteración 800: 1.764176\n",
      "Coste tras la iteración 900: 2.940846\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 520 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 1.028118\n",
      "Coste tras la iteración 200: 1.704558\n",
      "Coste tras la iteración 300: 2.882924\n",
      "Coste tras la iteración 400: 1.021868\n",
      "Coste tras la iteración 500: 1.712910\n",
      "Coste tras la iteración 600: 2.878112\n",
      "Coste tras la iteración 700: 1.015727\n",
      "Coste tras la iteración 800: 1.720995\n",
      "Coste tras la iteración 900: 2.873326\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 530 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 1.007050\n",
      "Coste tras la iteración 200: 1.663612\n",
      "Coste tras la iteración 300: 2.817150\n",
      "Coste tras la iteración 400: 1.001033\n",
      "Coste tras la iteración 500: 1.671483\n",
      "Coste tras la iteración 600: 2.812609\n",
      "Coste tras la iteración 700: 0.995121\n",
      "Coste tras la iteración 800: 1.679092\n",
      "Coste tras la iteración 900: 2.808086\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 540 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 0.986816\n",
      "Coste tras la iteración 200: 1.623819\n",
      "Coste tras la iteración 300: 2.753519\n",
      "Coste tras la iteración 400: 0.981020\n",
      "Coste tras la iteración 500: 1.631228\n",
      "Coste tras la iteración 600: 2.749238\n",
      "Coste tras la iteración 700: 0.975326\n",
      "Coste tras la iteración 800: 1.638380\n",
      "Coste tras la iteración 900: 2.744970\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 550 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 0.967352\n",
      "Coste tras la iteración 200: 1.585093\n",
      "Coste tras la iteración 300: 2.691879\n",
      "Coste tras la iteración 400: 0.961768\n",
      "Coste tras la iteración 500: 1.592058\n",
      "Coste tras la iteración 600: 2.687851\n",
      "Coste tras la iteración 700: 0.956282\n",
      "Coste tras la iteración 800: 1.598768\n",
      "Coste tras la iteración 900: 2.683827\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 560 ***\n",
      "Coste tras la iteración 0: 0.693147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coste tras la iteración 100: 0.948599\n",
      "Coste tras la iteración 200: 1.547347\n",
      "Coste tras la iteración 300: 2.632083\n",
      "Coste tras la iteración 400: 0.943217\n",
      "Coste tras la iteración 500: 1.553883\n",
      "Coste tras la iteración 600: 2.628300\n",
      "Coste tras la iteración 700: 0.937930\n",
      "Coste tras la iteración 800: 1.560168\n",
      "Coste tras la iteración 900: 2.624514\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 570 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 0.930498\n",
      "Coste tras la iteración 200: 1.510491\n",
      "Coste tras la iteración 300: 2.573989\n",
      "Coste tras la iteración 400: 0.925310\n",
      "Coste tras la iteración 500: 1.516612\n",
      "Coste tras la iteración 600: 2.570443\n",
      "Coste tras la iteración 700: 0.920213\n",
      "Coste tras la iteración 800: 1.522485\n",
      "Coste tras la iteración 900: 2.566887\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 580 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 0.912990\n",
      "Coste tras la iteración 200: 1.474429\n",
      "Coste tras la iteración 300: 2.517451\n",
      "Coste tras la iteración 400: 0.907989\n",
      "Coste tras la iteración 500: 1.480147\n",
      "Coste tras la iteración 600: 2.514138\n",
      "Coste tras la iteración 700: 0.903073\n",
      "Coste tras la iteración 800: 1.485620\n",
      "Coste tras la iteración 900: 2.510806\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 590 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 0.896016\n",
      "Coste tras la iteración 200: 1.439055\n",
      "Coste tras la iteración 300: 2.462324\n",
      "Coste tras la iteración 400: 0.891193\n",
      "Coste tras la iteración 500: 1.444381\n",
      "Coste tras la iteración 600: 2.459239\n",
      "Coste tras la iteración 700: 0.886453\n",
      "Coste tras la iteración 800: 1.449462\n",
      "Coste tras la iteración 900: 2.456124\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 600 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 0.879512\n",
      "Coste tras la iteración 200: 1.404249\n",
      "Coste tras la iteración 300: 2.408453\n",
      "Coste tras la iteración 400: 0.874862\n",
      "Coste tras la iteración 500: 1.409192\n",
      "Coste tras la iteración 600: 2.405592\n",
      "Coste tras la iteración 700: 0.870289\n",
      "Coste tras la iteración 800: 1.413890\n",
      "Coste tras la iteración 900: 2.402691\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 610 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 0.863409\n",
      "Coste tras la iteración 200: 1.369873\n",
      "Coste tras la iteración 300: 2.355670\n",
      "Coste tras la iteración 400: 0.858925\n",
      "Coste tras la iteración 500: 1.374439\n",
      "Coste tras la iteración 600: 2.353030\n",
      "Coste tras la iteración 700: 0.854513\n",
      "Coste tras la iteración 800: 1.378758\n",
      "Coste tras la iteración 900: 2.350337\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 620 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 0.847625\n",
      "Coste tras la iteración 200: 1.335751\n",
      "Coste tras la iteración 300: 2.303780\n",
      "Coste tras la iteración 400: 0.843304\n",
      "Coste tras la iteración 500: 1.339945\n",
      "Coste tras la iteración 600: 2.301360\n",
      "Coste tras la iteración 700: 0.839047\n",
      "Coste tras la iteración 800: 1.343888\n",
      "Coste tras la iteración 900: 2.298873\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 630 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 0.832063\n",
      "Coste tras la iteración 200: 1.301658\n",
      "Coste tras la iteración 300: 2.252546\n",
      "Coste tras la iteración 400: 0.827898\n",
      "Coste tras la iteración 500: 1.305481\n",
      "Coste tras la iteración 600: 2.250346\n",
      "Coste tras la iteración 700: 0.823792\n",
      "Coste tras la iteración 800: 1.309046\n",
      "Coste tras la iteración 900: 2.248061\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 640 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 0.816589\n",
      "Coste tras la iteración 200: 1.267276\n",
      "Coste tras la iteración 300: 2.201654\n",
      "Coste tras la iteración 400: 0.812578\n",
      "Coste tras la iteración 500: 1.270724\n",
      "Coste tras la iteración 600: 2.199676\n",
      "Coste tras la iteración 700: 0.808617\n",
      "Coste tras la iteración 800: 1.273903\n",
      "Coste tras la iteración 900: 2.197592\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 650 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 0.801004\n",
      "Coste tras la iteración 200: 1.232111\n",
      "Coste tras la iteración 300: 2.150644\n",
      "Coste tras la iteración 400: 0.797145\n",
      "Coste tras la iteración 500: 1.235175\n",
      "Coste tras la iteración 600: 2.148891\n",
      "Coste tras la iteración 700: 0.793324\n",
      "Coste tras la iteración 800: 1.237951\n",
      "Coste tras la iteración 900: 2.147007\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 660 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 0.784968\n",
      "Coste tras la iteración 200: 1.195301\n",
      "Coste tras la iteración 300: 2.098738\n",
      "Coste tras la iteración 400: 0.781262\n",
      "Coste tras la iteración 500: 1.197959\n",
      "Coste tras la iteración 600: 2.097220\n",
      "Coste tras la iteración 700: 0.777577\n",
      "Coste tras la iteración 800: 1.200295\n",
      "Coste tras la iteración 900: 2.095529\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 670 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 0.767757\n",
      "Coste tras la iteración 200: 1.154980\n",
      "Coste tras la iteración 300: 2.044302\n",
      "Coste tras la iteración 400: 0.764210\n",
      "Coste tras la iteración 500: 1.157183\n",
      "Coste tras la iteración 600: 2.043032\n",
      "Coste tras la iteración 700: 0.760653\n",
      "Coste tras la iteración 800: 1.158993\n",
      "Coste tras la iteración 900: 2.041516\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 680 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 0.746900\n",
      "Coste tras la iteración 200: 1.104784\n",
      "Coste tras la iteración 300: 1.981874\n",
      "Coste tras la iteración 400: 0.743540\n",
      "Coste tras la iteración 500: 1.106374\n",
      "Coste tras la iteración 600: 1.980848\n",
      "Coste tras la iteración 700: 0.740071\n",
      "Coste tras la iteración 800: 1.107320\n",
      "Coste tras la iteración 900: 1.979356\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 690 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 1.054410\n",
      "Coste tras la iteración 200: 1.059962\n",
      "Coste tras la iteración 300: 1.099283\n",
      "Coste tras la iteración 400: 1.115081\n",
      "Coste tras la iteración 500: 1.871311\n",
      "Coste tras la iteración 600: 0.816954\n",
      "Coste tras la iteración 700: 0.653484\n",
      "Coste tras la iteración 800: 1.897453\n",
      "Coste tras la iteración 900: 1.022002\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 700 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 0.922415\n",
      "Coste tras la iteración 200: 1.463253\n",
      "Coste tras la iteración 300: 1.746811\n",
      "Coste tras la iteración 400: 0.654426\n",
      "Coste tras la iteración 500: 1.706805\n",
      "Coste tras la iteración 600: 1.110596\n",
      "Coste tras la iteración 700: 1.802931\n",
      "Coste tras la iteración 800: 0.649085\n",
      "Coste tras la iteración 900: 1.614689\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 710 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 0.726895\n",
      "Coste tras la iteración 200: 0.894058\n",
      "Coste tras la iteración 300: 1.887664\n",
      "Coste tras la iteración 400: 0.895199\n",
      "Coste tras la iteración 500: 0.726860\n",
      "Coste tras la iteración 600: 1.324078\n",
      "Coste tras la iteración 700: 0.734402\n",
      "Coste tras la iteración 800: 0.984364\n",
      "Coste tras la iteración 900: 0.680729\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 720 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 0.661612\n",
      "Coste tras la iteración 200: 1.024447\n",
      "Coste tras la iteración 300: 1.124639\n",
      "Coste tras la iteración 400: 0.672504\n",
      "Coste tras la iteración 500: 1.762667\n",
      "Coste tras la iteración 600: 0.751560\n",
      "Coste tras la iteración 700: 0.690736\n",
      "Coste tras la iteración 800: 1.010384\n",
      "Coste tras la iteración 900: 1.667492\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 730 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 0.755559\n",
      "Coste tras la iteración 200: 1.048104\n",
      "Coste tras la iteración 300: 1.554932\n",
      "Coste tras la iteración 400: 0.658465\n",
      "Coste tras la iteración 500: 1.479219\n",
      "Coste tras la iteración 600: 1.067250\n",
      "Coste tras la iteración 700: 0.660028\n",
      "Coste tras la iteración 800: 0.922641\n",
      "Coste tras la iteración 900: 1.610729\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 740 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 0.683601\n",
      "Coste tras la iteración 200: 1.031274\n",
      "Coste tras la iteración 300: 0.901185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coste tras la iteración 400: 1.512886\n",
      "Coste tras la iteración 500: 0.892604\n",
      "Coste tras la iteración 600: 1.773365\n",
      "Coste tras la iteración 700: 0.869941\n",
      "Coste tras la iteración 800: 1.262979\n",
      "Coste tras la iteración 900: 0.766680\n",
      "Precisión de entrenamiento: 32.16666666666667 %\n",
      "Precisión de prueba: 32.0 %\n",
      "*** tasa = 750 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 0.673273\n",
      "Coste tras la iteración 200: 0.690584\n",
      "Coste tras la iteración 300: 1.687366\n",
      "Coste tras la iteración 400: 1.563592\n",
      "Coste tras la iteración 500: 0.992221\n",
      "Coste tras la iteración 600: 0.665556\n",
      "Coste tras la iteración 700: 0.671949\n",
      "Coste tras la iteración 800: 1.446725\n",
      "Coste tras la iteración 900: 0.668715\n",
      "Precisión de entrenamiento: 30.33333333333333 %\n",
      "Precisión de prueba: 31.25 %\n",
      "*** tasa = 760 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 0.871643\n",
      "Coste tras la iteración 200: 0.654046\n",
      "Coste tras la iteración 300: 0.814106\n",
      "Coste tras la iteración 400: 0.712181\n",
      "Coste tras la iteración 500: 1.595218\n",
      "Coste tras la iteración 600: 0.693117\n",
      "Coste tras la iteración 700: 0.932722\n",
      "Coste tras la iteración 800: 0.896732\n",
      "Coste tras la iteración 900: 0.829558\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 770 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 1.182952\n",
      "Coste tras la iteración 200: 0.824894\n",
      "Coste tras la iteración 300: 0.760476\n",
      "Coste tras la iteración 400: 1.383232\n",
      "Coste tras la iteración 500: 1.063179\n",
      "Coste tras la iteración 600: 0.821010\n",
      "Coste tras la iteración 700: 0.653627\n",
      "Coste tras la iteración 800: 0.661020\n",
      "Coste tras la iteración 900: 0.829219\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 780 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 1.343427\n",
      "Coste tras la iteración 200: 0.792882\n",
      "Coste tras la iteración 300: 1.621434\n",
      "Coste tras la iteración 400: 0.801052\n",
      "Coste tras la iteración 500: 0.673971\n",
      "Coste tras la iteración 600: 0.990838\n",
      "Coste tras la iteración 700: 0.675534\n",
      "Coste tras la iteración 800: 1.361869\n",
      "Coste tras la iteración 900: 0.770353\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 790 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 0.719508\n",
      "Coste tras la iteración 200: 0.655238\n",
      "Coste tras la iteración 300: 0.672441\n",
      "Coste tras la iteración 400: 0.918223\n",
      "Coste tras la iteración 500: 0.771367\n",
      "Coste tras la iteración 600: 1.564775\n",
      "Coste tras la iteración 700: 0.872312\n",
      "Coste tras la iteración 800: 1.359265\n",
      "Coste tras la iteración 900: 0.649905\n",
      "Precisión de entrenamiento: 34.66666666666667 %\n",
      "Precisión de prueba: 36.25000000000001 %\n",
      "*** tasa = 800 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 0.670385\n",
      "Coste tras la iteración 200: 1.539912\n",
      "Coste tras la iteración 300: 0.879459\n",
      "Coste tras la iteración 400: 0.705322\n",
      "Coste tras la iteración 500: 0.909386\n",
      "Coste tras la iteración 600: 1.109675\n",
      "Coste tras la iteración 700: 0.858093\n",
      "Coste tras la iteración 800: 0.653542\n",
      "Coste tras la iteración 900: 0.727328\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 810 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 1.514967\n",
      "Coste tras la iteración 200: 0.718466\n",
      "Coste tras la iteración 300: 0.718953\n",
      "Coste tras la iteración 400: 0.657519\n",
      "Coste tras la iteración 500: 1.282520\n",
      "Coste tras la iteración 600: 0.676016\n",
      "Coste tras la iteración 700: 0.939062\n",
      "Coste tras la iteración 800: 0.721071\n",
      "Coste tras la iteración 900: 1.415352\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 820 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 0.869738\n",
      "Coste tras la iteración 200: 0.915801\n",
      "Coste tras la iteración 300: 1.062693\n",
      "Coste tras la iteración 400: 0.653802\n",
      "Coste tras la iteración 500: 1.383036\n",
      "Coste tras la iteración 600: 0.827236\n",
      "Coste tras la iteración 700: 0.650256\n",
      "Coste tras la iteración 800: 0.648808\n",
      "Coste tras la iteración 900: 1.307059\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 830 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 0.678738\n",
      "Coste tras la iteración 200: 0.675737\n",
      "Coste tras la iteración 300: 0.671452\n",
      "Coste tras la iteración 400: 0.688992\n",
      "Coste tras la iteración 500: 0.688433\n",
      "Coste tras la iteración 600: 0.689383\n",
      "Coste tras la iteración 700: 0.665841\n",
      "Coste tras la iteración 800: 0.904775\n",
      "Coste tras la iteración 900: 0.648282\n",
      "Precisión de entrenamiento: 59.833333333333336 %\n",
      "Precisión de prueba: 65.0 %\n",
      "*** tasa = 840 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 0.901487\n",
      "Coste tras la iteración 200: 1.427503\n",
      "Coste tras la iteración 300: 1.429382\n",
      "Coste tras la iteración 400: 1.430763\n",
      "Coste tras la iteración 500: 1.431934\n",
      "Coste tras la iteración 600: 1.432973\n",
      "Coste tras la iteración 700: 1.433918\n",
      "Coste tras la iteración 800: 1.434790\n",
      "Coste tras la iteración 900: 1.435604\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 850 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 0.670711\n",
      "Coste tras la iteración 200: 0.711763\n",
      "Coste tras la iteración 300: 0.664269\n",
      "Coste tras la iteración 400: 0.653027\n",
      "Coste tras la iteración 500: 0.652394\n",
      "Coste tras la iteración 600: 0.652842\n",
      "Coste tras la iteración 700: 0.652546\n",
      "Coste tras la iteración 800: 0.735733\n",
      "Coste tras la iteración 900: 1.038773\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 860 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 0.756156\n",
      "Coste tras la iteración 200: 0.778175\n",
      "Coste tras la iteración 300: 0.691447\n",
      "Coste tras la iteración 400: 1.288870\n",
      "Coste tras la iteración 500: 0.684817\n",
      "Coste tras la iteración 600: 0.653987\n",
      "Coste tras la iteración 700: 0.688412\n",
      "Coste tras la iteración 800: 1.249593\n",
      "Coste tras la iteración 900: 1.050076\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 870 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 0.655065\n",
      "Coste tras la iteración 200: 1.255510\n",
      "Coste tras la iteración 300: 0.653070\n",
      "Coste tras la iteración 400: 1.342963\n",
      "Coste tras la iteración 500: 0.651463\n",
      "Coste tras la iteración 600: 0.671621\n",
      "Coste tras la iteración 700: 0.649361\n",
      "Coste tras la iteración 800: 0.649174\n",
      "Coste tras la iteración 900: 0.757154\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 880 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 0.863099\n",
      "Coste tras la iteración 200: 0.692054\n",
      "Coste tras la iteración 300: 0.661020\n",
      "Coste tras la iteración 400: 0.686748\n",
      "Coste tras la iteración 500: 0.652956\n",
      "Coste tras la iteración 600: 0.735176\n",
      "Coste tras la iteración 700: 0.649508\n",
      "Coste tras la iteración 800: 1.159363\n",
      "Coste tras la iteración 900: 0.869362\n",
      "Precisión de entrenamiento: 32.16666666666667 %\n",
      "Precisión de prueba: 32.0 %\n",
      "*** tasa = 890 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 0.655560\n",
      "Coste tras la iteración 200: 0.655305\n",
      "Coste tras la iteración 300: 0.683166\n",
      "Coste tras la iteración 400: 0.751907\n",
      "Coste tras la iteración 500: 0.677217\n",
      "Coste tras la iteración 600: 0.651385\n",
      "Coste tras la iteración 700: 0.695561\n",
      "Coste tras la iteración 800: 0.702157\n",
      "Coste tras la iteración 900: 0.828060\n",
      "Precisión de entrenamiento: 31.0 %\n",
      "Precisión de prueba: 32.0 %\n",
      "*** tasa = 900 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 0.727349\n",
      "Coste tras la iteración 200: 0.656041\n",
      "Coste tras la iteración 300: 0.827462\n",
      "Coste tras la iteración 400: 0.838195\n",
      "Coste tras la iteración 500: 0.802877\n",
      "Coste tras la iteración 600: 0.835346\n",
      "Coste tras la iteración 700: 0.651226\n",
      "Coste tras la iteración 800: 0.834804\n",
      "Coste tras la iteración 900: 0.672212\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 910 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 0.683367\n",
      "Coste tras la iteración 200: 0.838387\n",
      "Coste tras la iteración 300: 0.655512\n",
      "Coste tras la iteración 400: 0.654481\n",
      "Coste tras la iteración 500: 0.801184\n",
      "Coste tras la iteración 600: 0.754652\n",
      "Coste tras la iteración 700: 0.656431\n",
      "Coste tras la iteración 800: 0.686600\n",
      "Coste tras la iteración 900: 0.822403\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 920 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 0.798232\n",
      "Coste tras la iteración 200: 0.828850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coste tras la iteración 300: 0.686265\n",
      "Coste tras la iteración 400: 0.710068\n",
      "Coste tras la iteración 500: 0.742801\n",
      "Coste tras la iteración 600: 0.751335\n",
      "Coste tras la iteración 700: 0.779104\n",
      "Coste tras la iteración 800: 0.699962\n",
      "Coste tras la iteración 900: 0.652370\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 930 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 0.702038\n",
      "Coste tras la iteración 200: 0.712238\n",
      "Coste tras la iteración 300: 0.682606\n",
      "Coste tras la iteración 400: 0.670108\n",
      "Coste tras la iteración 500: 0.674948\n",
      "Coste tras la iteración 600: 0.661175\n",
      "Coste tras la iteración 700: 0.664353\n",
      "Coste tras la iteración 800: 0.701737\n",
      "Coste tras la iteración 900: 0.715934\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 940 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 0.662979\n",
      "Coste tras la iteración 200: 0.670762\n",
      "Coste tras la iteración 300: 0.660558\n",
      "Coste tras la iteración 400: 0.666804\n",
      "Coste tras la iteración 500: 0.661868\n",
      "Coste tras la iteración 600: 0.675648\n",
      "Coste tras la iteración 700: 0.665391\n",
      "Coste tras la iteración 800: 0.658457\n",
      "Coste tras la iteración 900: 0.661111\n",
      "Precisión de entrenamiento: 69.33333333333334 %\n",
      "Precisión de prueba: 70.5 %\n",
      "*** tasa = 950 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 0.665346\n",
      "Coste tras la iteración 200: 0.682943\n",
      "Coste tras la iteración 300: 0.663168\n",
      "Coste tras la iteración 400: 0.681679\n",
      "Coste tras la iteración 500: 0.661222\n",
      "Coste tras la iteración 600: 0.678442\n",
      "Coste tras la iteración 700: 0.658687\n",
      "Coste tras la iteración 800: 0.682573\n",
      "Coste tras la iteración 900: 0.658810\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 960 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 0.667435\n",
      "Coste tras la iteración 200: 0.679054\n",
      "Coste tras la iteración 300: 0.665101\n",
      "Coste tras la iteración 400: 0.677856\n",
      "Coste tras la iteración 500: 0.662964\n",
      "Coste tras la iteración 600: 0.676637\n",
      "Coste tras la iteración 700: 0.660974\n",
      "Coste tras la iteración 800: 0.675412\n",
      "Coste tras la iteración 900: 0.659110\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 970 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 0.673259\n",
      "Coste tras la iteración 200: 0.671798\n",
      "Coste tras la iteración 300: 0.670957\n",
      "Coste tras la iteración 400: 0.669736\n",
      "Coste tras la iteración 500: 0.669038\n",
      "Coste tras la iteración 600: 0.667411\n",
      "Coste tras la iteración 700: 0.668648\n",
      "Coste tras la iteración 800: 0.663175\n",
      "Coste tras la iteración 900: 0.670015\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 980 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 0.674540\n",
      "Coste tras la iteración 200: 0.673447\n",
      "Coste tras la iteración 300: 0.672385\n",
      "Coste tras la iteración 400: 0.671352\n",
      "Coste tras la iteración 500: 0.670348\n",
      "Coste tras la iteración 600: 0.669372\n",
      "Coste tras la iteración 700: 0.668422\n",
      "Coste tras la iteración 800: 0.667497\n",
      "Coste tras la iteración 900: 0.666598\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n",
      "*** tasa = 990 ***\n",
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 0.676247\n",
      "Coste tras la iteración 200: 0.675140\n",
      "Coste tras la iteración 300: 0.674065\n",
      "Coste tras la iteración 400: 0.673020\n",
      "Coste tras la iteración 500: 0.672005\n",
      "Coste tras la iteración 600: 0.671017\n",
      "Coste tras la iteración 700: 0.670057\n",
      "Coste tras la iteración 800: 0.669124\n",
      "Coste tras la iteración 900: 0.668215\n",
      "Precisión de entrenamiento: 70.0 %\n",
      "Precisión de prueba: 70.0 %\n"
     ]
    }
   ],
   "source": [
    "for i in range(10,1000,10):\n",
    "    print(\"*** tasa = \" + str(i) + \" ***\" )\n",
    "    d = modelo(CE_x2, CP_x2, CE_y2, CP_y2, num_iter = 1000, tasa = 1/(i*1000), print_cost = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que la red neuronal se contruyó con sólo una neurona, la regresión logística tradicional supera a la red neuronal, no obstante, si se buscaran otras configuraciones para la red neuronal, como aumentar el número de neuronas y adicionar capas ocultas, sería posible encuentrar mejores resultados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
